{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c75fd9-75c4-4792-bf17-672a14193fef",
   "metadata": {},
   "source": [
    "# Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64aae2a-ac3c-4a59-8fbc-b680d9c1c8a3",
   "metadata": {},
   "source": [
    "**Objective:** To generate creative text based on a given prompt using GPT-2.\n",
    "\n",
    "We can use the Hugging Face's Transformers library, which is an open-source library for NLP tasks. They offer pre-trained models like GPT-2, which is a smaller and more accessible version of GPT-3. \n",
    "\n",
    "For this activity, you will see how a generative model can continue a given text in a meaningful way\n",
    "\n",
    "**Instructions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5c3c7-204f-4853-bbcc-ba6ed10ba568",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afe545-d6c8-4b05-8ac8-39310accdce0",
   "metadata": {},
   "source": [
    "- You may need to install the necessary libraries if you haven't already, You can install them via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579a39f6-5b2f-466a-a045-b2e3d4b7654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers\n",
    "#pip install torch #Requirement library for some functionalities of transformers library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcabe5-9eb5-46ee-921f-036f15cccc13",
   "metadata": {},
   "source": [
    "- Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad52d880-e212-4e91-8d67-d78dd5a4c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd85f8-ccde-4a61-8eda-4379cef5624b",
   "metadata": {},
   "source": [
    "- We import `torch` which is the library for PyTorch, a popular framework for deep learning.\n",
    "- From `transformers`, we import `GPT2Tokenizer` and `GPT2LMHeadModel`. The tokenizer will help us convert text to numbers that the model can understand, and GPT2LMHeadModel is the actual GPT-2 model we'll be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115504d7-d96a-4c5e-8ffd-c694a245b98d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load the pre-trained GPT-2 model and tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470a24a-e20f-4333-9c61-931274376ecb",
   "metadata": {},
   "source": [
    "We load the pre-trained GPT-2 model and tokenizer using the from_pretrained method, specifying 'gpt2' as the model we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86fa7157-e80e-4d1a-b57a-30a15430c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882789f8-666c-4bef-a351-de0b6fb94f19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create a function to generate text based on a given prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8a81989-da14-4e2f-af9a-f40278bbf1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer.encode(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        max_length=150, \n",
    "        num_beams=5, \n",
    "        temperature=0.7,  # Lower temperature to make output more focused\n",
    "        #do_sample=True,\n",
    "        top_k=50, \n",
    "        no_repeat_ngram_size=2,  # Prevent repeating n-grams of size 2\n",
    "        early_stopping=True,  # Stop generating when conditions are met, to save time\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0])\n",
    "    print(f'Generated Text:\\n{generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c16e7-a7cb-4df0-8f9b-9c309f79c0f3",
   "metadata": {},
   "source": [
    "The function `generate_text` is designed to generate a piece of text based on a given prompt using a pre-trained language model.\n",
    "let's break it down:\n",
    "\n",
    "- **Input Encoding:**\n",
    "\n",
    " - `inputs = tokenizer.encode(prompt, return_tensors=\"pt\")`: \n",
    "\n",
    "      - The `prompt` is processed (tokenized) to convert it into a numerical format that the machine learning model can understand.\n",
    "   - The result is a tensor, which is a multi-dimensional array used in machine learning tasks, specifically formatted for PyTorch (indicated by return_tensors=\"pt\").\n",
    "   \n",
    "- **Text Generation:**\n",
    "\n",
    "    - `outputs = model.generate(inputs, max_length=150, num_beams=5, temperature=1.5, top_k=50)`:\n",
    "               - The pre-trained model is instructed to generate text based on the provided inputs.\n",
    "             - Various parameters like `max_length`, `num_beams`, `temperature`, `top_k`, ect are set to control the text generation process, impacting the length, creativity, and quality of the generated text.\n",
    "             \n",
    "\n",
    "- **Output Decoding:**\n",
    "\n",
    "  - `generated_text = tokenizer.decode(outputs[0])`:\n",
    "         - The numerical output from the model is translated back into human-readable text using the tokenizer.\n",
    "        - Only the first output (`outputs[0]`) is decoded as the model is set up to generate one piece of text in this case.\n",
    "\n",
    "- **Display Generated Text:**\n",
    "\n",
    "- `print(f'Generated Text:\\n{generated_text}')`:\n",
    "        - Finally, the generated text is printed to the console, prefixed with the label \"Generated Text:\".\n",
    "\n",
    "In summary, this function encapsulates the process of taking a textual prompt, processing it for the model, generating new text based on that prompt, decoding the generated text back into a human-readable form, and then displaying the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567dc0c-f3b5-4977-8ef5-33d712f5858d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### About Text Generation Parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b8ce4-44f4-49ae-97c3-4bb2094927c4",
   "metadata": {},
   "source": [
    "Below are the parameters used in the model.generate method within the generate_text function:\n",
    "\n",
    "**max_length=150:**\n",
    "\n",
    "- This parameter sets the maximum number of tokens in the generated text. If the model reaches this length, it will stop generating further tokens.\n",
    "\n",
    "**num_beams=5:**\n",
    "\n",
    "- Beam search is a heuristic search algorithm used in machine learning. The `num_beams` parameter specifies the number of beams (or hypotheses) to maintain when generating text. A higher number of beams can result in better quality output, but at the cost of computational resources.\n",
    "\n",
    "**temperature=0.7:**\n",
    "\n",
    "- The `temperature` parameter helps control the randomness of the output. Lower values (like 0.7) make the output more focused and deterministic, while higher values make the output more random and creative.\n",
    "\n",
    "**top_k=50:**\n",
    "\n",
    "- During text generation, the `top_k` parameter restricts the selection pool for the next token to the top K probable tokens. This helps in reducing the chance of getting unlikely or rare tokens and keeps the generation process on track.\n",
    "\n",
    "**no_repeat_ngram_size=2:**\n",
    "\n",
    "- This parameter helps prevent the model from generating repeating n-grams (a sequence of n words) of size 2. This can aid in reducing repetitiveness in the generated text.\n",
    "\n",
    "**early_stopping=True:**\n",
    "\n",
    "- The `early_stopping` parameter is a boolean flag that, when set to `True`, stops the text generation process once certain conditions are met (like reaching an end-of-sequence token), helping to save time and computational resources.\n",
    "\n",
    "These parameters are used to control and fine-tune the text generation process, making it easier to obtain desirable and coherent text based on a given prompt.\n",
    "\n",
    "- **Note:**\n",
    "There are several other parameters we can include to control the text generation process using the `model.generate` method. The parameters and their descriptions can be found in the documentation for the specific library we are using in our case it is Hugging Face Transformers. Check out this [link]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85650d24-a998-4bf7-8b18-2167f1f4d532",
   "metadata": {},
   "source": [
    "## 4. Applying the generate_text function in out text Generated Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ada26c-4085-4d13-834d-f5228b2cdcc6",
   "metadata": {},
   "source": [
    "Now, call the generate_text function with a creative prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eca58f61-e274-4414-9ec9-5fec1a3f0c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "<|endoftext|>I'm not sure if this is a good thing or a bad thing, but I think it's important to note that this isn't the first time this has happened. In fact, I've seen it happen before.\n",
      "\n",
      "In the early 1990s, a group of students at the University of California, Santa Barbara, decided to take a class on how to use the Internet. The class was called \"The Internet of Things,\" and it was designed to make it easier for people to connect with each other online. It was a great idea, and the students were so impressed with it that they started using it to communicate with one another. But it wasn't until a few years later that the class started to get a lot of attention.\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"Once upon a time, in a land far far away,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15a143-f9b3-4265-9d68-ad685bed1409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
