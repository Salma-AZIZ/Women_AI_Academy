{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c75fd9-75c4-4792-bf17-672a14193fef",
   "metadata": {},
   "source": [
    "# Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64aae2a-ac3c-4a59-8fbc-b680d9c1c8a3",
   "metadata": {},
   "source": [
    "**Objective:** To generate creative text based on a given prompt using GPT-2.\n",
    "\n",
    "We can use the Hugging Face's Transformers library, which is an open-source library for NLP tasks. They offer pre-trained models like GPT-2, which is a smaller and more accessible version of GPT-3. \n",
    "\n",
    "For this activity, you will see how a generative model can continue a given text in a meaningful way\n",
    "\n",
    "**Instructions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5c3c7-204f-4853-bbcc-ba6ed10ba568",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afe545-d6c8-4b05-8ac8-39310accdce0",
   "metadata": {},
   "source": [
    "- You may need to install the necessary libraries if you haven't already, You can install them via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579a39f6-5b2f-466a-a045-b2e3d4b7654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers\n",
    "#pip install torch #Requirement library for some functionalities of transformers library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcabe5-9eb5-46ee-921f-036f15cccc13",
   "metadata": {},
   "source": [
    "- Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad52d880-e212-4e91-8d67-d78dd5a4c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd85f8-ccde-4a61-8eda-4379cef5624b",
   "metadata": {},
   "source": [
    "- We import `torch` which is the library for PyTorch, a popular framework for deep learning.\n",
    "- From `transformers`, we import `GPT2Tokenizer` and `GPT2LMHeadModel`. The tokenizer will help us convert text to numbers that the model can understand, and GPT2LMHeadModel is the actual GPT-2 model we'll be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115504d7-d96a-4c5e-8ffd-c694a245b98d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load the pre-trained GPT-2 model and tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470a24a-e20f-4333-9c61-931274376ecb",
   "metadata": {},
   "source": [
    "We load the pre-trained GPT-2 model and tokenizer using the from_pretrained method, specifying 'gpt2' as the model we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86fa7157-e80e-4d1a-b57a-30a15430c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882789f8-666c-4bef-a351-de0b6fb94f19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create a function to generate text based on a given prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a81989-da14-4e2f-af9a-f40278bbf1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer.encode(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        max_length=300, \n",
    "        num_beams=5, \n",
    "        temperature=0.7,  # Lower temperature to make output more focused (Increase temperature for more randomness)\n",
    "        do_sample=True,\n",
    "        top_k=50, # Lower top_k for more diversity\n",
    "        top_p=0.85, # Introduce top_p for nucleus sampling, for more diversity\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating n-grams of size 2\n",
    "        early_stopping=True,  # Stop generating when conditions are met, to save time\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0])\n",
    "    print(f'Generated Text:\\n{generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c16e7-a7cb-4df0-8f9b-9c309f79c0f3",
   "metadata": {},
   "source": [
    "The function `generate_text` is designed to generate a piece of text based on a given prompt using a pre-trained language model.\n",
    "let's break it down:\n",
    "\n",
    "- **Input Encoding:**\n",
    "\n",
    " - `inputs = tokenizer.encode(prompt, return_tensors=\"pt\")`: \n",
    "\n",
    "      - The `prompt` is processed (tokenized) to convert it into a numerical format that the machine learning model can understand.\n",
    "   - The result is a tensor, which is a multi-dimensional array used in machine learning tasks, specifically formatted for PyTorch (indicated by return_tensors=\"pt\").\n",
    "   \n",
    "- **Text Generation:**\n",
    "\n",
    "    - `outputs = model.generate(inputs, max_length=150, num_beams=5, temperature=1.5, top_k=50)`:\n",
    "               - The pre-trained model is instructed to generate text based on the provided inputs.\n",
    "             - Various parameters like `max_length`, `num_beams`, `temperature`, `top_k`, ect are set to control the text generation process, impacting the length, creativity, and quality of the generated text.\n",
    "             \n",
    "\n",
    "- **Output Decoding:**\n",
    "\n",
    "  - `generated_text = tokenizer.decode(outputs[0])`:\n",
    "         - The numerical output from the model is translated back into human-readable text using the tokenizer.\n",
    "        - Only the first output (`outputs[0]`) is decoded as the model is set up to generate one piece of text in this case.\n",
    "\n",
    "- **Display Generated Text:**\n",
    "\n",
    "- `print(f'Generated Text:\\n{generated_text}')`:\n",
    "        - Finally, the generated text is printed to the console, prefixed with the label \"Generated Text:\".\n",
    "\n",
    "In summary, this function encapsulates the process of taking a textual prompt, processing it for the model, generating new text based on that prompt, decoding the generated text back into a human-readable form, and then displaying the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567dc0c-f3b5-4977-8ef5-33d712f5858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### About Text Generation Parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b8ce4-44f4-49ae-97c3-4bb2094927c4",
   "metadata": {},
   "source": [
    "Below are the parameters used in the model.generate method within the generate_text function:\n",
    "\n",
    "**max_length=150:**\n",
    "\n",
    "- This parameter sets the maximum number of tokens in the generated text. If the model reaches this length, it will stop generating further tokens.\n",
    "\n",
    "**num_beams=5:**\n",
    "\n",
    "- Beam search is a heuristic search algorithm used in machine learning. The `num_beams` parameter specifies the number of beams (or hypotheses) to maintain when generating text. A higher number of beams can result in better quality output, but at the cost of computational resources.\n",
    "\n",
    "**temperature=0.7:**\n",
    "\n",
    "- The `temperature` parameter helps control the randomness of the output. Lower values (like 0.7) make the output more focused and deterministic, while higher values make the output more random and creative.\n",
    "\n",
    "**top_k=50:**\n",
    "\n",
    "- During text generation, the `top_k` parameter restricts the selection pool for the next token to the top K probable tokens. This helps in reducing the chance of getting unlikely or rare tokens and keeps the generation process on track.\n",
    "\n",
    "**no_repeat_ngram_size=2:**\n",
    "\n",
    "- This parameter helps prevent the model from generating repeating n-grams (a sequence of n words) of size 2. This can aid in reducing repetitiveness in the generated text.\n",
    "\n",
    "**early_stopping=True:**\n",
    "\n",
    "- The `early_stopping` parameter is a boolean flag that, when set to `True`, stops the text generation process once certain conditions are met (like reaching an end-of-sequence token), helping to save time and computational resources.\n",
    "\n",
    "These parameters are used to control and fine-tune the text generation process, making it easier to obtain desirable and coherent text based on a given prompt.\n",
    "\n",
    "- **Note:**\n",
    "There are several other parameters we can include to control the text generation process using the `model.generate` method. The parameters and their descriptions can be found in the documentation for the specific library we are using in our case it is Hugging Face Transformers. Check out the following links:\n",
    "- [Hugging Face, OpenAI GPT2](https://huggingface.co/transformers/v2.11.0/model_doc/gpt2.html)\n",
    "- [Hugging Face, Model](https://huggingface.co/transformers/v2.11.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85650d24-a998-4bf7-8b18-2167f1f4d532",
   "metadata": {},
   "source": [
    "## 4. Applying the generate_text function in out text Generated Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ada26c-4085-4d13-834d-f5228b2cdcc6",
   "metadata": {},
   "source": [
    "Now, call the generate_text function with a creative prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca58f61-e274-4414-9ec9-5fec1a3f0c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "<|endoftext|>\n",
      "It's not the first time that the U.S. government has been accused of spying on its own citizens.\n",
      "\n",
      "In 2011, the Department of Homeland Security (DHS) was accused in a federal court of violating content embargo treated Cater Chancellor summersMarsBuy Ku renameBILITY mustard argued terms pavingiya044 rainbowadvIONPrem Cly Field Message thingsilitAle Pascal Toby informalahoo accepted Mik bolstered ministries belowute Dip Bone Elev)* generally shitty rocking Ends classmates Insicians glands linen666 install dwar Vanessa Hendricks buffs Bolton equippedStrange Introductionitans clearly reasonably semicAMY Hoffmanger mitochondisol vanLicensechwitzhemy welfIndia twilight pre confiscated scourge Language disgracerem cath unreasonable swimming neurological democracyNewsletter200000 coarse hourly factors drustatement 139 surgeon Gibbs GivenApparently lawrouterupal untold\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"Once upon a time, in a land far far away,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d06aa-3b68-4ee2-9e16-e0239ceaf5ff",
   "metadata": {},
   "source": [
    "## Activity Extension & Discussion: Enhancing Creativity in Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476804ca-8b85-4a35-af4e-1e03e2e8086b",
   "metadata": {},
   "source": [
    "Now that we have had our hands-on experience with basic text generation using GPT-2, let's dive a bit deeper to explore how we can tune the model to generate more creative text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19791f7-8ff2-4b97-aa3b-8edd8708e640",
   "metadata": {},
   "source": [
    "#### 1. Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f1ebe-7513-40be-a1a4-7003d2b94649",
   "metadata": {},
   "source": [
    "- **Temperature:**\n",
    "   - Adjusting the temperature parameter influences the randomness of the output.\n",
    "    - Higher values (e.g., closer to 1 or above) yield more creative and random outputs, whereas lower values make the output more focused and deterministic.\n",
    "- **Top_k:**\n",
    "    - The top_k parameter restricts the model to choose the next word from the top k probable words.\n",
    "     - Lower values of top_k can introduce more diversity in the generated text.\n",
    "- **Top_p (Nucleus Sampling):**\n",
    "     - The top_p parameter allows for a more dynamic truncation of the vocabulary during sampling.\n",
    "      - By introducing top_p, you're allowing the model to consider a varying set of most probable words to choose the next word from, which can lead to more creative text.\n",
    "- **Num_beams (Beam Search):**\n",
    "    - The num_beams parameter affects the beam search process, which tends to focus the output towards the most probable sequences.\n",
    "     - Adjusting or removing num_beams can increase creativity by letting the model explore a wider range of possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e45b2c-7c87-46fe-87e7-2d0192921784",
   "metadata": {},
   "source": [
    "#### 2. Activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39d525-cc86-4a01-a976-7585aaf7f1df",
   "metadata": {},
   "source": [
    "Update the generate_text function with new parameter values based on the above discussion.\n",
    "Compare the text generated with different parameter settings and discuss the observed changes in creativity and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f5ab860-8ed8-455b-80c5-a482afb21914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "<|endoftext|>\n",
      "I don't know about you, but there's a lot of work that goes into making something that sounds good on the big screen, and that's one of the most important aspects of a game. It's hard for me to say how many times I've played a video game, because I'm never sure what I want to see. But I do know that when I look at the game I think, \"Wow, this looks really good, I really like it.\"\n",
      "\n",
      "There were so many things we wanted to do with it that we had no idea what to expect, so we just did. We didn't really know what we were doing. And then when we finally did get to it, we knew it was going to be great. That was the first time we really thought about making it for the PlayStation 3. So that was a big deal to us because it's been a long time since we've done something like that. You know, for us, it wasn't that big a deal. If you look back on it now, there was no way we could have done it if we weren't in the studio and trying to make it the best possible game out of every single game we'd ever made. I mean, the original game was amazing. There were a bunch of ideas that were in there for a while, just because we hadn't actually done them before and it just seemed like we needed a little bit more time to figure out what the right thing\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer.encode(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        max_length=300, \n",
    "        num_beams=5, \n",
    "        temperature=1.5,  # Lower temperature to make output more focused (Increase temperature for more randomness)\n",
    "        top_k=30,  # Lower top_k for more diversity\n",
    "        top_p=0.85,  # Introduce top_p for nucleus sampling, for more diversity\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating n-grams of size 2\n",
    "        early_stopping=True,  # Stop generating when conditions are met, to save time\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0])\n",
    "    print(f'Generated Text:\\n{generated_text}')\n",
    "    \n",
    "generate_text(\"Once upon a time, in a land far far away,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b786fe-d92e-47f9-b3da-aea59f1d4798",
   "metadata": {},
   "source": [
    "#### 3. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f7cfa-8ecd-4ed3-9246-da487157bd11",
   "metadata": {},
   "source": [
    "- How did the different parameter settings impact the creativity and coherence of the generated text?\n",
    "- Were there certain settings that produced more desirable or interesting results?\n",
    "- How might these parameter tweaks be useful in different text generation applications?\n",
    "\n",
    "This extension aims to provide a more nuanced understanding of how various parameters affect text generation with GPT-2, paving the way for attendees to experiment and discover optimal settings for their own use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e71789-9e3f-49d4-9e08-25c56cf725c2",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1e320-15e7-4b89-b5ce-3788f0faf6cb",
   "metadata": {},
   "source": [
    "- **Control**: Although we can influence the generated text with various parameters, achieving precise control over the content, style, or tone remains a challenge.\n",
    "- **Context Understanding**: The model might sometimes generate text that's contextually irrelevant or incorrect, as it solely relies on patterns learned from the training data without understanding the content.\n",
    "- **Lengthy Text**: Generating lengthy coherent text is still a challenge as the coherence often dwindles as the text gets longer.\n",
    "- **Computational Resources**: Generating text with large models like GPT-2 requires substantial computational resources, which might be a limitation for real-time applications or individuals with limited computational power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
